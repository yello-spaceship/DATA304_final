ì¢‹ì•„ ðŸ™‚ ì•„ëž˜ **í•˜ë‚˜ì§œë¦¬ ìµœì¢… README**ë§Œ ê·¸ëŒ€ë¡œ ë³µë¶™í•˜ë©´ ë¼!

```markdown
# Amazon Product Classification â€” Hybrid Taxo-Fusion Pipeline

This repository contains a **hybrid multi-label classification pipeline** for Amazon product review data.
The final method combines:
- **SBERT embeddings + PCA**
- **Taxonomy-aware class similarity (TaxoClass + hierarchy smoothing)**
- **Keyword/TF-IDF heuristic features**
- **Ensemble classifier (MLP + Logistic Regression)**
- **Hierarchy-consistent label refinement**

> âš ï¸ This project is designed to run in **Jupyter Notebook (.ipynb)** and was executed in a **GPU environment**.

---

## Repository Structure

```

.
â”œâ”€â”€ README.md
â”œâ”€â”€ final_code.ipynb         # Main training + inference pipeline (produces final submission)
â””â”€â”€ gpt_usage.ipynb          # GPT-based strong silver label + class description generation

````

---

## 0) Quick Start (Recommended Order)

1. **Download dataset** and set `DATA_BASE` in both notebooks  
2. (Optional but recommended) Run **`gpt_usage.ipynb`** to generate GPT resources  
3. Run **`final_code.ipynb`** to train + infer and generate submission CSV  
4. Submit the generated CSV (e.g., `amz_final4.csv`)

---

## 1) Requirements

### Python Packages
Main dependencies (non-exhaustive):
- `torch`
- `transformers`
- `sentence-transformers` (model: `sentence-transformers/all-MiniLM-L12-v2`)
- `scikit-learn`
- `pandas`, `numpy`
- `networkx`
- `tqdm`

Install in notebook environment if needed:
```bash
pip install torch transformers sentence-transformers scikit-learn pandas numpy networkx tqdm
````

---

## 2) Data Preparation

### 2.1 Set Base Path (`DATA_BASE`)

In the current code, paths are set like:

```python
DATA_BASE = "/mnt/custom-file-systems/s3/shared/seonga"
```

âœ… After downloading your dataset, **change `DATA_BASE`** to your local directory, e.g.:

```python
DATA_BASE = "/path/to/your/data"
```

> You must update `DATA_BASE` in **both** `gpt_usage.ipynb` and `final_code.ipynb`.

### 2.2 Required Files in `DATA_BASE`

Your `DATA_BASE` should include:

**Core dataset files**

* `train_corpus.txt`
* `test_corpus.txt`
* `stopword.txt`
* `classes.txt`
* `class_related_keywords.txt`
* `class_hierarchy.txt`
* `ranked_keywords_top10_final.csv`

**GPT outputs (generated by `gpt_usage.ipynb`)**

* `gpt_class_descriptions_100calls.json`
* `2000_gpt_667call.csv`

---

## 3) GPT Usage (Silver Labels + Class Descriptions)

### 3.1 Why this notebook is needed

The main pipeline (`final_code.ipynb`) can leverage GPT outputs for:

1. **Class descriptions** â†’ used to build taxonomy-aware class embeddings
2. **Strong silver labels** â†’ used to train seed models on high-confidence pseudo labels

So, for best results, run **`gpt_usage.ipynb` first**.

### 3.2 Set OpenAI API Key

Before running `gpt_usage.ipynb`, set:

```bash
export OPENAI_API_KEY="YOUR_KEY"
```

### 3.3 Outputs produced by `gpt_usage.ipynb`

Make sure these files are generated (or paths match your `DATA_BASE`):

* `gpt_class_descriptions_100calls.json`
* `2000_gpt_667call.csv`

---

## 4) Running the Main Pipeline (`final_code.ipynb`)

### 4.1 Run in Jupyter

Both notebooks are `.ipynb`, so run them in Jupyter:

* (Recommended) `gpt_usage.ipynb`
* `final_code.ipynb`

### 4.2 GPU Execution

This pipeline was executed on **GPU**.
The code automatically checks device:

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"[SETUP] Running on device: {device}")
```

âœ… You should see:

* `[SETUP] Running on device: cuda`

If it prints `cpu`, runtime may become significantly slower.

---

## 5) Output

### 5.1 Final submission CSV

`final_code.ipynb` generates the final submission file (path may vary by your settings):

* `FINAL_SUBMISSION = f"{DATA_BASE}/result/amz_final4.csv"`

### 5.2 Format

Output CSV format:

* header: `id,label`
* label: comma-separated **top-k** class IDs (multi-label)

Example:

```
id,label
123,10,42,77
124,5,19,21
...
```

---

## 6) Notes / Tips

* This is a **multi-label** setting (top-3 labels).
* The pipeline includes:

  * minimum label constraint (at least 2 labels)
  * hierarchical consistency enforcement (add parents if needed)
* Hyperparameters are controlled near the bottom of `final_code.ipynb`, e.g.:

  * `W_MLP_RATIO`, `W_LOGREG_RATIO`
  * `ALPHA_MLP_TAXO`
  * `W_MODEL_TO_HEURISTIC_FINAL`
  * `MLP_HIDDEN_LAYERS`

---

## 7) Troubleshooting

### 7.1 Path errors

Most errors come from wrong file paths.
Check:

* `DATA_BASE`
* required file names
* output directories (e.g., `result/` exists)

### 7.2 GPT outputs missing

If you skip `gpt_usage.ipynb`, ensure:

* `CLASS_DESCRIPTION_PATH` falls back to class names (the code supports fallback)
* `STRONG_SILVER_PATH` is either generated or replaced with a valid file

### 7.3 Slow runtime

If running on CPU, expect large slowdown.
Try a GPU runtime or reduce data size during debugging.

---

## Acknowledgement

This codebase implements a hybrid approach combining taxonomy knowledge, embedding-based similarity, and heuristic keyword signals for robust Amazon product classification.

```

