{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import warnings \n",
    "import sklearn.multiclass\n",
    "\n",
    "# ğŸš¨ğŸš¨ğŸš¨ í™˜ê²½ ì„¤ì • ë° ê²½ê³  ë¬´ì‹œ (ì˜¤ë¥˜ ë°©ì§€) ğŸš¨ğŸš¨ğŸš¨\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn.multiclass')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message='Label not .* is present in all training examples.')\n",
    "\n",
    "\n",
    "# --- ğŸš¨ íŒŒì¼ ê²½ë¡œ ì„¤ì • (Amazon í™˜ê²½ ê²½ë¡œ ìœ ì§€) ---\n",
    "DATA_BASE = \"/mnt/custom-file-systems/s3/shared/seonga\"\n",
    "\n",
    "TFIDF_RANKING_PATH = f\"{DATA_BASE}/ranked_keywords_top10_final.csv\" \n",
    "CLASS_DESCRIPTION_PATH = f\"{DATA_BASE}/gpt_class_descriptions_100calls.json\" \n",
    "# ============================================\n",
    "# 0. Paths & Globals\n",
    "# ============================================\n",
    "\n",
    "TRAIN_CORPUS = f\"{DATA_BASE}/train_corpus.txt\"\n",
    "TEST_CORPUS  = f\"{DATA_BASE}/test_corpus.txt\"\n",
    "\n",
    "STOPWORD_PATH = f\"{DATA_BASE}/stopword.txt\"\n",
    "CLASS_PATH    = f\"{DATA_BASE}/classes.txt\"\n",
    "KEYWORD_PATH  = f\"{DATA_BASE}/class_related_keywords.txt\"\n",
    "HIER_PATH     = f\"{DATA_BASE}/class_hierarchy.txt\"\n",
    "\n",
    "STRONG_SILVER_PATH = f\"{DATA_BASE}/2000_gpt_667call.csv\"\n",
    "TRAIN_SILVER_PATH  = f\"{DATA_BASE}/train_silver_heuristic_01.csv\"\n",
    "FINAL_SUBMISSION   = f\"{DATA_BASE}/result/amz_final4.csv\" # íŒŒì¼ëª… ë³€ê²½\n",
    "\n",
    "TOTAL_CLASSES = 531 \n",
    "SEED = 42\n",
    "PCA_COMPONENTS = 256 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[SETUP] Running on device: {device}\")\n",
    "\n",
    "# ğŸš¨ ëª¨ë¸: all-MiniLM-L12-v2 ì‚¬ìš©\n",
    "BERT_MODEL_NAME = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME).to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "pca = PCA(n_components=PCA_COMPONENTS, random_state=SEED) \n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 1. Loaders & Utilities\n",
    "# ============================================\n",
    "def load_stopwords(path):\n",
    "    stop = set()\n",
    "    if not os.path.exists(path): return stop\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for w in f:\n",
    "            w = w.strip().lower()\n",
    "            if w: stop.add(w)\n",
    "    return stop\n",
    "\n",
    "def load_class_names(path):\n",
    "    name2id = {}\n",
    "    if not os.path.exists(path): return name2id\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(maxsplit=1) \n",
    "            if len(parts) >= 2:\n",
    "                cid = parts[0]; cname = parts[1]\n",
    "                if cid.isdigit(): name2id[cname] = int(cid)\n",
    "    return name2id\n",
    "\n",
    "def load_keywords(path, name2id):\n",
    "    kw_map = {}\n",
    "    if not os.path.exists(path): return kw_map\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line: continue\n",
    "            cname, kws = line.split(\":\", 1)\n",
    "            cname = cname.strip(); kws = [k.strip().lower() for k in kws.split(\",\") if k.strip()]\n",
    "            if cname in name2id: kw_map[name2id[cname]] = kws\n",
    "    return kw_map\n",
    "\n",
    "def load_hierarchy(path, name2id):\n",
    "    parent_map, child_map = {}, {}\n",
    "    G = nx.DiGraph()\n",
    "    if not os.path.exists(path): return parent_map, child_map, G\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = re.split(r'[\\s,]+', line.strip()) \n",
    "            if len(parts) >= 2:\n",
    "                p, c = parts[0], parts[1]\n",
    "                try:\n",
    "                    pid, cid = int(p), int(c)\n",
    "                    parent_map.setdefault(pid, set()).add(cid)\n",
    "                    child_map.setdefault(cid, set()).add(pid)\n",
    "                    G.add_edge(pid, cid)\n",
    "                except ValueError: pass\n",
    "    return parent_map, child_map, G\n",
    "\n",
    "def load_corpus(path, desc=\"Loading Corpus\"):\n",
    "    pid2text = {}\n",
    "    if not os.path.exists(path): print(f\"Error: Corpus file not found at {path}\"); return pid2text\n",
    "    num_lines = sum(1 for _ in open(path, \"r\", encoding=\"utf-8\"))\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, total=num_lines, desc=desc):\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2: pid2text[parts[0]] = parts[1]\n",
    "            elif len(parts) == 1: pid2text[str(len(pid2text))] = parts[0]\n",
    "            \n",
    "    if not any(pid.isdigit() for pid in pid2text.keys()) or ' ' in next(iter(pid2text.keys()), ''):\n",
    "          return {str(i): text for i, text in enumerate(pid2text.values())}\n",
    "\n",
    "    return pid2text\n",
    "\n",
    "def load_silver_labels(path):\n",
    "    pid2label = {}\n",
    "    if not os.path.exists(path): print(f\"Error: Silver label file not found at {path}\"); return pid2label\n",
    "    num_lines = sum(1 for _ in open(path, \"r\", encoding=\"utf-8\"))\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f); header = next(reader, None)\n",
    "        for row in tqdm(reader, total=num_lines-1, desc=f\"Loading Labels from {os.path.basename(path)}\"):\n",
    "            if len(row) != 2: continue\n",
    "            pid, labels = row\n",
    "            labels = labels.strip()\n",
    "            if not labels: pid2label[pid] = []; continue\n",
    "            pid2label[pid] = [int(x) for x in labels.split(\",\") if x.strip().isdigit()]\n",
    "    return pid2label\n",
    "\n",
    "def load_tfidf_ranking(path):\n",
    "    if not os.path.exists(path): \n",
    "        print(f\"Error: TFIDF Ranking file not found at {path}\"); \n",
    "        return {}\n",
    "        \n",
    "    df = pd.read_csv(path)\n",
    "    tfidf_map = {}\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading TFIDF Map\"):\n",
    "        pid = str(row['doc_id'])\n",
    "        try:\n",
    "            kws_str = row['keywords'].strip('[]')\n",
    "            scores_str = row['scores'].strip('[]')\n",
    "\n",
    "            kws = [k.strip().replace(\"'\", \"\").replace('\"', '').replace(' ', '') for k in kws_str.split(',') if k.strip()]\n",
    "            scores = [float(s.strip()) for s in scores_str.split(',') if s.strip()]\n",
    "            \n",
    "            if len(kws) == len(scores):\n",
    "                tfidf_map[pid] = {'keywords': kws, 'scores': scores}\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "    return tfidf_map\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_bert_embeddings(text_list, tokenizer, model, device, batch_size=64, description=\"SBERT Embedding\"): \n",
    "    all_embs = []\n",
    "    text_list = [str(t) if pd.isna(t) else t for t in text_list]\n",
    "    \n",
    "    batches = list(range(0, len(text_list), batch_size))\n",
    "    for i in tqdm(batches, desc=description):\n",
    "        batch_texts = text_list[i : i + batch_size]\n",
    "        \n",
    "        enc = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, \n",
    "                              max_length=512, padding=True).to(device)\n",
    "        \n",
    "        out = model(**enc)\n",
    "        \n",
    "        cls_embs = out.last_hidden_state[:, 0, :]\n",
    "        all_embs.append(cls_embs.cpu())\n",
    "        \n",
    "    if all_embs: \n",
    "        return torch.cat(all_embs, dim=0).numpy()\n",
    "    else: \n",
    "        return np.empty((0, model.config.hidden_size))\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def clean_text_to_tokens(text, stopwords):\n",
    "    text = re.sub(r\"[^A-Za-z0-9]+\", \" \", text.lower())\n",
    "    return [t for t in text.split() if t not in stopwords]\n",
    "\n",
    "# í‚¤ì›Œë“œ ê¸°ë°˜ íŠ¹ì§• ë²¡í„° (X_KEYWORD) ìƒì„± (ë¶„ë¥˜ê¸° ì…ë ¥ìš©)\n",
    "def build_keyword_features(pids, pid2text, keyword_map, tfidf_ranking_map, num_classes, stopwords):\n",
    "    print(\"\\n[Feature Upgrade] Building Keyword Feature Vector (X_KEYWORD)...\")\n",
    "    \n",
    "    X_KEYWORD = np.zeros((len(pids), num_classes), dtype=np.float32)\n",
    "    pid_to_index = {pid: i for i, pid in enumerate(pids)}\n",
    "    \n",
    "    for pid in tqdm(pids, desc=\"Generating Keyword Features\"):\n",
    "        idx = pid_to_index[pid]\n",
    "        text = pid2text.get(pid, \"\")\n",
    "        \n",
    "        tokens = clean_text_to_tokens(text, stopwords)\n",
    "        token_set = set(tokens)\n",
    "        \n",
    "        tfidf_data = tfidf_ranking_map.get(pid)\n",
    "        tfidf_kws = tfidf_data['keywords'] if tfidf_data else []\n",
    "        tfidf_scores = tfidf_data['scores'] if tfidf_data else []\n",
    "        \n",
    "        for cid, kws in keyword_map.items():\n",
    "            if cid >= num_classes: continue\n",
    "            \n",
    "            score = 0.0\n",
    "            for kw in kws:\n",
    "                if kw in token_set:\n",
    "                    score += 1.0\n",
    "                    \n",
    "                    try:\n",
    "                        kw_index = tfidf_kws.index(kw)\n",
    "                        score += tfidf_scores[kw_index] * 0.5 \n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            \n",
    "            X_KEYWORD[idx, cid] = score\n",
    "            \n",
    "    max_val = np.max(X_KEYWORD, axis=0, keepdims=True)\n",
    "    max_val[max_val == 0] = 1.0 \n",
    "    X_KEYWORD = X_KEYWORD / max_val\n",
    "    \n",
    "    return X_KEYWORD\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# TaxoClass ì§€ì‹ ë¡œë“œ ë° íŠ¹ì§• êµ¬ì¶• í•¨ìˆ˜ (í‚¤ì›Œë“œ í†µí•©)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def load_class_descriptions(path, id_to_name, keyword_map):\n",
    "    \"\"\" GPTë¡œ ìƒì„±ëœ í´ë˜ìŠ¤ ì„¤ëª…ì„ ë¡œë“œí•˜ê³ , í‚¤ì›Œë“œ ì •ë³´ë¥¼ í†µí•©í•©ë‹ˆë‹¤. \"\"\"\n",
    "    \n",
    "    descriptions = id_to_name.copy()\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            loaded_descriptions = {\n",
    "                int(k): v\n",
    "                for k, v in data.items() \n",
    "                if isinstance(v, str) and v.strip()\n",
    "            }\n",
    "            \n",
    "            for cid, name in id_to_name.items():\n",
    "                if cid in loaded_descriptions:\n",
    "                    descriptions[cid] = loaded_descriptions[cid]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load GPT descriptions. Using default names. Error: {e}\")\n",
    "            pass\n",
    "            \n",
    "    final_map = {}\n",
    "    for cid, text in descriptions.items():\n",
    "        keyword_list = keyword_map.get(cid)\n",
    "        \n",
    "        if keyword_list:\n",
    "            keywords_str = \", \".join(keyword_list)\n",
    "            cleaned_text = text.rstrip('.')\n",
    "            final_map[cid] = f\"{cleaned_text}. Keywords: {keywords_str}.\"\n",
    "        else:\n",
    "            final_map[cid] = text\n",
    "            \n",
    "    return final_map\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_class_features(id_to_description, bert_tokenizer, bert_model, device, G, num_classes, pca_model):\n",
    "    \"\"\"\n",
    "    TaxoClassì˜ í´ë˜ìŠ¤ íŠ¹ì§• ë²¡í„° (Description + Contextual)ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    class_texts = [\"\" for _ in range(num_classes)]\n",
    "    for cid, text in id_to_description.items():\n",
    "        if cid < num_classes:\n",
    "            class_texts[cid] = text\n",
    "\n",
    "    print(\"\\n[TaxoClass] 1/3: Generating Base Embeddings from Descriptions...\")\n",
    "    base_embeddings = get_bert_embeddings(class_texts, bert_tokenizer, bert_model, device, \n",
    "                                             batch_size=64, description=\"Encoding Class Descriptions\")\n",
    "    \n",
    "    print(\"[TaxoClass] 2/3: Applying Contextual Embedding (Hierarchy Smoothing)...\")\n",
    "    context_embeddings = base_embeddings.copy()\n",
    "    \n",
    "    try:\n",
    "        for node in reversed(list(nx.topological_sort(G))):\n",
    "            if node >= num_classes: continue\n",
    "            \n",
    "            for parent in G.predecessors(node):\n",
    "                if parent < num_classes:\n",
    "                    context_embeddings[parent] += base_embeddings[node] * 0.2\n",
    "                    context_embeddings[parent] /= 1.2 \n",
    "\n",
    "    except nx.NetworkXUnfeasible:\n",
    "        print(\"WARNING: Hierarchy Graph has cycles, skipping contextual smoothing.\")\n",
    "\n",
    "    print(\"[TaxoClass] 3/3: Applying PCA Transform...\")\n",
    "    X_CLASS_FEATURES = pca_model.transform(context_embeddings)\n",
    "    \n",
    "    return X_CLASS_FEATURES\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# GNN ê°œë… ì ìš© í•¨ìˆ˜ (Score Refinement)\n",
    "# ----------------------------------------------------\n",
    "def apply_gnn_score_refinement(scores_matrix, G, N_classes):\n",
    "    refined_scores = scores_matrix.copy()\n",
    "    \n",
    "    try:\n",
    "        for node in reversed(list(nx.topological_sort(G))):\n",
    "            if node >= N_classes: continue\n",
    "            \n",
    "            for parent in G.predecessors(node):\n",
    "                if parent < N_classes:\n",
    "                    refined_scores[:, parent] = np.maximum(\n",
    "                        refined_scores[:, parent], \n",
    "                        refined_scores[:, node]\n",
    "                    )\n",
    "    except nx.NetworkXUnfeasible:\n",
    "        pass\n",
    "\n",
    "    return refined_scores\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# íœ´ë¦¬ìŠ¤í‹± ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ (TFIDF, Keyword) - ìµœì¢… ìœµí•©ìš©\n",
    "# ----------------------------------------------------\n",
    "def score_text_with_tfidf(\n",
    "    pid, tokens, keyword_map, parent_map, child_map, tfidf_ranking_map,\n",
    "    w_exact=1.0, w_tfidf_boost=0.5, w_emotion=0.0, w_hier_child=0.5\n",
    "):\n",
    "    \"\"\" TFIDF ë° í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ì˜ ë…ë¦½ì ì¸ íœ´ë¦¬ìŠ¤í‹± ì ìˆ˜ ê³„ì‚° \"\"\"\n",
    "    scores = defaultdict(float); token_set = set(tokens)\n",
    "    \n",
    "    for cid, kws in keyword_map.items():\n",
    "        for kw in kws:\n",
    "            if kw in token_set:\n",
    "                scores[cid] += w_exact\n",
    "\n",
    "    if pid in tfidf_ranking_map:\n",
    "        tfidf_data = tfidf_ranking_map[pid]\n",
    "        tfidf_kws = tfidf_data['keywords']\n",
    "        tfidf_scores = tfidf_data['scores']\n",
    "        \n",
    "        for kw, score in zip(tfidf_kws, tfidf_scores):\n",
    "            for cid, kws in keyword_map.items():\n",
    "                if kw in kws:\n",
    "                    scores[cid] += w_tfidf_boost * score\n",
    "\n",
    "    # ê³„ì¸µ êµ¬ì¡° ì „íŒŒ (ìì‹ -> ë¶€ëª¨)\n",
    "    for cid, sc in list(scores.items()):\n",
    "        if sc <= 0: continue\n",
    "        if cid in child_map:\n",
    "            for p in child_map[cid]: scores[p] += sc * w_hier_child \n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ğŸš¨ ìƒˆ í•¨ìˆ˜: ìµœì†Œ ë ˆì´ë¸” ê°œìˆ˜ ì œì•½ ê¸°ë°˜ ì •ì œ\n",
    "# ----------------------------------------------------\n",
    "def refine_labels_with_min_constraint(sorted_scores, confidence_threshold, min_labels=2, max_labels=3):\n",
    "    \"\"\"\n",
    "    ìµœì¢… ì˜ˆì¸¡ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„, ìµœì†Œ 2ê°œ, ìµœëŒ€ 3ê°œì˜ ë ˆì´ë¸”ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "    - 2ê°œ ë¯¸ë§Œì´ ì„ê³„ê°’ì„ ë„˜ìœ¼ë©´, ìƒìœ„ 2ê°œëŠ” ê°•ì œë¡œ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    selected = []\n",
    "    \n",
    "    # 1. ì„ê³„ê°’ì„ ë„˜ëŠ” ë ˆì´ë¸” ì„ íƒ (ìµœëŒ€ 3ê°œ)\n",
    "    for cid, sc in sorted_scores:\n",
    "        if len(selected) >= max_labels: break\n",
    "        if sc >= confidence_threshold:\n",
    "            selected.append(cid)\n",
    "    \n",
    "    # 2. ìµœì†Œ ê°œìˆ˜ ì œì•½ ì¡°ê±´ ì ìš© (ìµœì†Œ 2ê°œ)\n",
    "    if len(selected) < min_labels:\n",
    "        top_k_candidates = sorted_scores[:max_labels]\n",
    "        \n",
    "        forced_selection = []\n",
    "        for i, (cid, sc) in enumerate(top_k_candidates):\n",
    "            if len(forced_selection) >= min_labels: break\n",
    "            # ì ìˆ˜ê°€ 0ì´ ì•„ë‹Œ ìœ íš¨í•œ í´ë˜ìŠ¤ë§Œ ê³ ë ¤\n",
    "            if sc > 1e-6 and cid not in forced_selection: \n",
    "                 forced_selection.append(cid)\n",
    "        \n",
    "        final_selection = list(set(selected + forced_selection))\n",
    "        \n",
    "        # ì ìˆ˜ ìˆœìœ¼ë¡œ ë‹¤ì‹œ ì •ë ¬ (ì¤‘ìš”)\n",
    "        score_dict = {cid: sc for cid, sc in sorted_scores}\n",
    "        final_selection.sort(key=lambda cid: score_dict.get(cid, 0.0), reverse=True)\n",
    "        \n",
    "        selected = final_selection[:max_labels]\n",
    "        \n",
    "    if not selected:\n",
    "        selected = [0]\n",
    "        \n",
    "    return selected\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ğŸš¨ ìƒˆ í•¨ìˆ˜: ê³„ì¸µ êµ¬ì¡° ì¼ê´€ì„± í™•ë³´ ë¡œì§ (Silver Label ìˆ˜ì •ìš©)\n",
    "# ----------------------------------------------------\n",
    "def enforce_hierarchical_consistency(\n",
    "    selected_labels, parent_map, score_dict, max_labels\n",
    "):\n",
    "    \"\"\"\n",
    "    ì„ íƒëœ ë ˆì´ë¸” ì§‘í•©ì— ëŒ€í•´ ê³„ì¸µì  ì¼ê´€ì„±ì„ í™•ë³´í•˜ê³ , ë ˆì´ë¸” ìˆ˜ê°€ ì´ˆê³¼ë˜ë©´ ì ìˆ˜ê°€ ë‚®ì€ ë ˆì´ë¸”ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # 1. ì¼ê´€ì„± í™•ë³´ë¥¼ ìœ„í•´ ì¶”ê°€í•´ì•¼ í•  ë¶€ëª¨ ë…¸ë“œ ì°¾ê¸°\n",
    "    labels_to_add = set()\n",
    "    current_labels = set(selected_labels)\n",
    "    \n",
    "    for cid in selected_labels:\n",
    "        # cidì˜ ë¶€ëª¨ë“¤ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "        for parent_cid in parent_map.get(cid, set()):\n",
    "            # ë¶€ëª¨ê°€ í˜„ì¬ ë ˆì´ë¸” ì§‘í•©ì— ì—†ìœ¼ë©´ ì¶”ê°€ ëª©ë¡ì— ë„£ìŠµë‹ˆë‹¤.\n",
    "            if parent_cid not in current_labels:\n",
    "                labels_to_add.add(parent_cid)\n",
    "\n",
    "    if not labels_to_add:\n",
    "        return selected_labels\n",
    "        \n",
    "    # 2. ë¶€ëª¨ ë…¸ë“œ ì¶”ê°€\n",
    "    final_labels_set = current_labels.union(labels_to_add)\n",
    "    final_labels_list = list(final_labels_set)\n",
    "\n",
    "    # 3. ë ˆì´ë¸” ìˆ˜ê°€ max_labelsë¥¼ ì´ˆê³¼í•˜ë©´ ë‚®ì€ ì ìˆ˜ ìˆœìœ¼ë¡œ ì œê±°\n",
    "    if len(final_labels_list) > max_labels:\n",
    "        \n",
    "        # ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì •ë ¬: ì ìˆ˜ê°€ ë†’ê±°ë‚˜, ì ìˆ˜ê°€ ê°™ìœ¼ë©´ ì›ë³¸ ë ˆì´ë¸”(selected_labels)ì— ìˆëŠ” ê²ƒì„ ìš°ì„  (ì¶”ê°€ëœ ë¶€ëª¨ê°€ ì œê±°ë  í™•ë¥  ë†’ì„)\n",
    "        def sort_key(cid):\n",
    "            # (ì ìˆ˜, ì›ë³¸ ë ˆì´ë¸”ì— í¬í•¨ ì—¬ë¶€)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n",
    "            return (score_dict.get(cid, 0.0), cid in current_labels)\n",
    "\n",
    "        final_labels_list.sort(key=sort_key, reverse=True)\n",
    "        \n",
    "        # ìµœëŒ€ ë ˆì´ë¸” ê°œìˆ˜ì— ë§ê²Œ ìë¦…ë‹ˆë‹¤.\n",
    "        final_labels_list = final_labels_list[:max_labels]\n",
    "        \n",
    "    return final_labels_list\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. Hybrid Taxo-Fusion Pipeline (í†µí•© ì‹¤í–‰)\n",
    "# ============================================\n",
    "\n",
    "def run_integrated_hybrid_pipeline(\n",
    "    train_corpus_path, test_corpus_path, strong_silver_path, final_silver_path,\n",
    "    name2id, keyword_map, parent_map, child_map, G, final_submission_path,\n",
    "    tfidf_ranking_map, \n",
    "    w_mlp_ratio=0.9, w_logreg_ratio=0.1, \n",
    "    alpha_mlp_taxo=0.2, \n",
    "    top_k=3,\n",
    "    mlp_confidence_threshold=0.85,\n",
    "    W_MODEL_TO_HEURISTIC=0.9,\n",
    "    # ğŸš¨ MLP ë³µì¡ë„ ë³€ê²½ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì¶”ê°€\n",
    "    MLP_HIDDEN_LAYERS=(256, 128)\n",
    "):\n",
    "    \n",
    "    W_HEURISTIC = 1.0 - W_MODEL_TO_HEURISTIC # 0.1\n",
    "    MIN_LABELS = 2\n",
    "    MAX_LABELS = top_k # 3\n",
    "    \n",
    "    print(f\"\\n--- [START] Hybrid Pipeline (Final Fusion: Model * {W_MODEL_TO_HEURISTIC} + Heuristic * {W_HEURISTIC}) ---\")\n",
    "\n",
    "    # --- [STEP 1 & 2] íŠ¹ì§• ì¶”ì¶œ ë° PCA ì ìš© ---\n",
    "    print(\"\\n[STEP 1 & 2] íŠ¹ì§• ì¶”ì¶œ ë° PCA ì ìš©\")\n",
    "\n",
    "    stopwords = load_stopwords(STOPWORD_PATH)\n",
    "    pid2text = load_corpus(train_corpus_path, desc=\"Loading Train Corpus for Fusion\")\n",
    "    strong_labels = load_silver_labels(strong_silver_path)\n",
    "\n",
    "    all_pids = list(pid2text.keys())\n",
    "    all_texts = list(pid2text.values())\n",
    "    \n",
    "    X_BERT_ALL = get_bert_embeddings(all_texts, bert_tokenizer, bert_model, device, description=\"Encoding All Train Reviews (X_BERT)\")\n",
    "    \n",
    "    strong_pids = [pid for pid in strong_labels.keys() if pid in pid2text]\n",
    "    train_targets_strong = [strong_labels[pid] for pid in strong_pids]\n",
    "    \n",
    "    strong_indices = [all_pids.index(pid) for pid in strong_pids]\n",
    "    X_BERT_STRONG = X_BERT_ALL[strong_indices]\n",
    "    \n",
    "    pca.fit(X_BERT_STRONG) \n",
    "    X_PCA_ALL = pca.transform(X_BERT_ALL) \n",
    "    X_PCA_STRONG = X_PCA_ALL[strong_indices]\n",
    "\n",
    "    num_classes = max(name2id.values()) + 1 if name2id else 1\n",
    "    mlb = MultiLabelBinarizer(classes=list(range(num_classes)))\n",
    "    Y_strong = mlb.fit_transform(train_targets_strong)\n",
    "    if hasattr(Y_strong, 'toarray'): Y_strong = Y_strong.toarray()\n",
    "    \n",
    "    \n",
    "    # --- [STEP 2.5] TaxoClass íŠ¹ì§• ë° í‚¤ì›Œë“œ íŠ¹ì§• êµ¬ì¶• (ì…ë ¥ íŠ¹ì§•) ---\n",
    "    print(\"\\n[STEP 2.5] ğŸ§  TaxoClass íŠ¹ì§• êµ¬ì¶• ë° Fusion Input ìƒì„±\")\n",
    "    \n",
    "    id_to_name = {v: k for k, v in name2id.items()}\n",
    "    id_to_description = load_class_descriptions(CLASS_DESCRIPTION_PATH, id_to_name, keyword_map)\n",
    "    X_CLASS_FEATURES = build_class_features(id_to_description, bert_tokenizer, bert_model, device, G, num_classes, pca)\n",
    "    \n",
    "    S_Taxo_ALL = cosine_similarity(X_PCA_ALL, X_CLASS_FEATURES)\n",
    "    S_Taxo_STRONG = S_Taxo_ALL[strong_indices] \n",
    "    S_Taxo_refined = apply_gnn_score_refinement(S_Taxo_ALL, G, num_classes)\n",
    "    \n",
    "    X_KEYWORD_ALL = build_keyword_features(all_pids, pid2text, keyword_map, tfidf_ranking_map, num_classes, stopwords)\n",
    "    X_KEYWORD_STRONG = X_KEYWORD_ALL[strong_indices]\n",
    "\n",
    "    # Fusion íŠ¹ì§• í™•ì¥: X_PCA, S_Taxo, X_KEYWORD\n",
    "    X_FUSION_ALL = np.concatenate([X_PCA_ALL, S_Taxo_ALL, X_KEYWORD_ALL], axis=1)\n",
    "    X_FUSION_STRONG = np.concatenate([X_PCA_STRONG, S_Taxo_STRONG, X_KEYWORD_STRONG], axis=1)\n",
    "    \n",
    "    print(f\"  --> Fusion Input Dimension: {X_FUSION_ALL.shape[1]} ({PCA_COMPONENTS} + {num_classes} + {num_classes})\")\n",
    "\n",
    "\n",
    "    # --- [STEP 3] 1ì°¨ ì•™ìƒë¸” í•™ìŠµ ë° S_MLP_DUMP ìƒì„± ---\n",
    "    print(\"\\n[STEP 3] 1ì°¨ ì•™ìƒë¸” í•™ìŠµ (Seed Model) - í™•ì¥ëœ Fusion Input ì‚¬ìš©\")\n",
    "\n",
    "    # ğŸš¨ MLP ë³µì¡ë„ ë³€ê²½: (256, 128) -> (512, 256)\n",
    "    base_clf_mlp = MLPClassifier(hidden_layer_sizes=MLP_HIDDEN_LAYERS, max_iter=500, alpha=5e-3, solver='adam', random_state=SEED, early_stopping=False)\n",
    "    ovr_clf_mlp_1st = OneVsRestClassifier(base_clf_mlp, n_jobs=-1)\n",
    "    ovr_clf_mlp_1st.fit(X_FUSION_STRONG, Y_strong)\n",
    "    S_MLP_DUMP = ovr_clf_mlp_1st.predict_proba(X_FUSION_ALL)\n",
    "    \n",
    "    base_clf_logreg = LogisticRegression(solver='saga', C=1.0, max_iter=1000, n_jobs=-1, random_state=SEED, penalty='l2', tol=1e-3)\n",
    "    ovr_clf_logreg_1st = OneVsRestClassifier(base_clf_logreg, n_jobs=-1)\n",
    "    ovr_clf_logreg_1st.fit(X_FUSION_STRONG, Y_strong)\n",
    "    S_LogReg = ovr_clf_logreg_1st.predict_proba(X_FUSION_ALL)\n",
    "\n",
    "    # ğŸš¨ ì•™ìƒë¸” ë¹„ìœ¨ ë³€ê²½: 0.9 -> 0.95\n",
    "    S_Model_Ensemble = (w_mlp_ratio * S_MLP_DUMP + w_logreg_ratio * S_LogReg)\n",
    "    print(f\"  --> 1ì°¨ ëª¨ë¸ ì•™ìƒë¸” ì™„ë£Œ (MLP:{w_mlp_ratio}, LogReg:{w_logreg_ratio}).\")\n",
    "\n",
    "\n",
    "    # --- [STEP 4] Pure Model Fusion Silver Label ìƒì„± (íœ´ë¦¬ìŠ¤í‹± ìµœì¢… ìœµí•© ì ìš© ë° êµ¬ì¡°ì  ë³´ì •) ---\n",
    "    print(\"\\n[STEP 4] ğŸ’¡ Pure Model Fusion Silver Label ìƒì„± (êµ¬ì¡°ì  ë³´ì • ì ìš©)\")\n",
    "\n",
    "    # ğŸš¨ Taxo ìœµí•© ë¹„ìœ¨ ë³€ê²½: alpha_mlp_taxo ì‚¬ìš©\n",
    "    S_Model_Ensemble_GNN = apply_gnn_score_refinement(S_Model_Ensemble, G, num_classes)\n",
    "    S_MLP_Taxo_Hybrid = (alpha_mlp_taxo * S_Model_Ensemble_GNN) + ((1.0 - alpha_mlp_taxo) * S_Taxo_refined)\n",
    "    S_MLP_Taxo_Hybrid_GNN = apply_gnn_score_refinement(S_MLP_Taxo_Hybrid, G, num_classes)\n",
    "\n",
    "    CONFIDENCE_THRESHOLD = 0.15\n",
    "    all_final_labels_map = {}\n",
    "    \n",
    "    for idx, (pid, text) in tqdm(enumerate(zip(all_pids, all_texts)), total=len(all_pids), desc=\"  Fusing Scores for Silver Label Generation\"):\n",
    "        final_scores = defaultdict(float)\n",
    "        \n",
    "        # ğŸš¨ íœ´ë¦¬ìŠ¤í‹± ì ìˆ˜ ê³„ì‚° ë° ìœµí•©\n",
    "        tokens = clean_text_to_tokens(text, stopwords)\n",
    "        heuristic_scores_raw = score_text_with_tfidf(\n",
    "            pid, tokens, keyword_map, parent_map, child_map, tfidf_ranking_map, w_emotion=0.0\n",
    "        )\n",
    "        \n",
    "        heuristic_scores = np.zeros(num_classes)\n",
    "        for cid, score in heuristic_scores_raw.items():\n",
    "             if cid < num_classes: heuristic_scores[cid] = score\n",
    "\n",
    "        if np.max(heuristic_scores) > 0:\n",
    "            heuristic_scores = heuristic_scores / np.max(heuristic_scores)\n",
    "\n",
    "\n",
    "        s_model_taxo = S_MLP_Taxo_Hybrid_GNN[idx] \n",
    "        \n",
    "        # ìµœì¢… ìœµí•©: Model * 0.9 + Heuristic * 0.1\n",
    "        final_hybrid_scores = (W_MODEL_TO_HEURISTIC * s_model_taxo) + (W_HEURISTIC * heuristic_scores)\n",
    "\n",
    "\n",
    "        for cid in range(num_classes):\n",
    "            final_scores[cid] = final_hybrid_scores[cid]\n",
    "\n",
    "        if final_scores:\n",
    "            sorted_scores = sorted(final_scores.items(), key=lambda x: -x[1])\n",
    "            score_dict = {cid: sc for cid, sc in sorted_scores}\n",
    "            \n",
    "            # 1. ìµœì†Œ ë ˆì´ë¸” ì œì•½ ê¸°ë°˜ ì„ íƒ\n",
    "            selected_initial = refine_labels_with_min_constraint(\n",
    "                sorted_scores, CONFIDENCE_THRESHOLD, min_labels=MIN_LABELS, max_labels=MAX_LABELS\n",
    "            )\n",
    "            \n",
    "            # 2. ê³„ì¸µì  ì¼ê´€ì„± ê¸°ë°˜ ë³´ì • ë° ìµœëŒ€ ë ˆì´ë¸” ìˆ˜ ì¬ì¡°ì •\n",
    "            selected = enforce_hierarchical_consistency(\n",
    "                selected_initial, parent_map, score_dict, MAX_LABELS\n",
    "            )\n",
    "            \n",
    "        else: selected = [0]\n",
    "\n",
    "        all_final_labels_map[pid] = selected\n",
    "\n",
    "    # Silver Label CSV ì €ì¥\n",
    "    print(f\"  [SAVE] Pure Model Fusion Silver Labels ì €ì¥ ì¤‘: {final_silver_path}\")\n",
    "    with open(final_silver_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"pid\", \"labels\"])\n",
    "        for pid, labs in tqdm(all_final_labels_map.items(), total=len(all_final_labels_map), desc=\"Writing Final Silver CSV\"):\n",
    "            w.writerow([pid, \",\".join(map(str, labs))])\n",
    "    print(\"  --> Pure Model Fusion Silver Labels ì €ì¥ ì™„ë£Œ.\")\n",
    "\n",
    "\n",
    "    # --- [STEP 5] ìµœì¢… ì•™ìƒë¸” í•™ìŠµ (MLP Confidence í•„í„°ë§ ì ìš© ë° ì¬í•™ìŠµ) ---\n",
    "    print(\"\\n[STEP 5] ìµœì¢… ì•™ìƒë¸” í•™ìŠµ (MLP Confidence í•„í„°ë§ ì ìš© ë° ì¬í•™ìŠµ)\")\n",
    "\n",
    "    final_silver_train = load_silver_labels(final_silver_path)\n",
    "    \n",
    "    S_MLP_MAX_CONF = np.max(S_MLP_DUMP, axis=1) \n",
    "    \n",
    "    filtered_indices = []\n",
    "    all_pids_map = {pid: i for i, pid in enumerate(all_pids)}\n",
    "\n",
    "    for pid, labs in final_silver_train.items():\n",
    "        if not labs: continue\n",
    "            \n",
    "        idx = all_pids_map.get(pid)\n",
    "        if idx is None: continue \n",
    "        \n",
    "        if S_MLP_MAX_CONF[idx] >= mlp_confidence_threshold:\n",
    "            filtered_indices.append(idx)\n",
    "        \n",
    "\n",
    "    X_final_train_filtered = X_FUSION_ALL[filtered_indices]\n",
    "    training_pids_filtered = [all_pids[i] for i in filtered_indices]\n",
    "    train_targets_final_filtered = [final_silver_train[pid] for pid in training_pids_filtered]\n",
    "    \n",
    "    Y_final_filtered = mlb.transform(train_targets_final_filtered)\n",
    "    if hasattr(Y_final_filtered, 'toarray'): Y_final_filtered = Y_final_filtered.toarray()\n",
    "    \n",
    "    print(f\"  --> í•„í„°ë§ëœ ìµœì¢… í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_final_train_filtered.shape[0]} / {X_FUSION_ALL.shape[0]}\")\n",
    "    \n",
    "    if X_final_train_filtered.shape[0] == 0:\n",
    "        print(\"  âŒ WARNING: í•„í„°ë§ í›„ ìœ íš¨í•œ í•™ìŠµ ìƒ˜í”Œì´ ì—†ì–´ ì˜ˆì¸¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        return \n",
    "        \n",
    "    # ğŸš¨ MLP ë³µì¡ë„ ë³€ê²½ ì ìš©\n",
    "    base_clf_mlp_final = MLPClassifier(hidden_layer_sizes=MLP_HIDDEN_LAYERS, max_iter=1000, alpha=5e-3, solver='adam', random_state=SEED, early_stopping=False)\n",
    "    ovr_clf_mlp_final = OneVsRestClassifier(base_clf_mlp_final, n_jobs=-1)\n",
    "    ovr_clf_mlp_final.fit(X_final_train_filtered, Y_final_filtered) \n",
    "    \n",
    "    base_clf_logreg_final = LogisticRegression(solver='saga', C=1.0, max_iter=3000, n_jobs=-1, random_state=SEED, penalty='l2', tol=1e-3)\n",
    "    ovr_clf_logreg_final = OneVsRestClassifier(base_clf_logreg_final, n_jobs=-1)\n",
    "    ovr_clf_logreg_final.fit(X_final_train_filtered, Y_final_filtered)\n",
    "    \n",
    "    print(\"  --> ìµœì¢… MLP/LogReg ì•™ìƒë¸” í•™ìŠµ ì™„ë£Œ (MLP Confidence í•„í„°ë§ ê¸°ë°˜).\")\n",
    "\n",
    "\n",
    "    # --- [STEP 6] ìµœì¢… ì˜ˆì¸¡ ë° íŒŒì¼ ì €ì¥ (íœ´ë¦¬ìŠ¤í‹± ìµœì¢… ìœµí•© ì ìš©) ---\n",
    "    print(\"\\n[STEP 6] ìµœì¢… ì˜ˆì¸¡ ë° íŒŒì¼ ì €ì¥ (íœ´ë¦¬ìŠ¤í‹± ìµœì¢… ìœµí•© ì ìš©)\")\n",
    "\n",
    "    test_corpus = load_corpus(test_corpus_path, desc=\"Loading Test Corpus for Prediction\")\n",
    "    test_pids = list(test_corpus.keys())\n",
    "    test_texts = [test_corpus[pid] for pid in test_pids]\n",
    "    \n",
    "    X_BERT_TEST = get_bert_embeddings(test_texts, bert_tokenizer, bert_model, device, description=\"Encoding Test Reviews (X_BERT)\")\n",
    "    X_PCA_TEST = pca.transform(X_BERT_TEST) \n",
    "    S_Taxo_Test = cosine_similarity(X_PCA_TEST, X_CLASS_FEATURES) \n",
    "    \n",
    "    X_KEYWORD_TEST = build_keyword_features(test_pids, test_corpus, keyword_map, tfidf_ranking_map, num_classes, stopwords)\n",
    "    \n",
    "    X_FUSION_TEST = np.concatenate([X_PCA_TEST, S_Taxo_Test, X_KEYWORD_TEST], axis=1) \n",
    "    \n",
    "    mlp_probas = ovr_clf_mlp_final.predict_proba(X_FUSION_TEST) \n",
    "    logreg_probas = ovr_clf_logreg_final.predict_proba(X_FUSION_TEST) \n",
    "    \n",
    "    # ğŸš¨ ì•™ìƒë¸” ë¹„ìœ¨ ë³€ê²½ ì ìš©\n",
    "    ensemble_probas = (w_mlp_ratio * mlp_probas + w_logreg_ratio * logreg_probas)\n",
    "    \n",
    "    S_Taxo_Test_refined = apply_gnn_score_refinement(S_Taxo_Test, G, num_classes)\n",
    "    \n",
    "    # 1ì°¨ ìœµí•© (Model + Taxo)\n",
    "    decision_scores_hybrid = (alpha_mlp_taxo * ensemble_probas) + ((1.0 - alpha_mlp_taxo) * S_Taxo_Test_refined)\n",
    "    \n",
    "    # ğŸš¨ íœ´ë¦¬ìŠ¤í‹± ì ìˆ˜ ê³„ì‚° ë° ìµœì¢… ìœµí•© (W_HEURISTIC = 0.1)\n",
    "    final_hybrid_probas = decision_scores_hybrid.copy()\n",
    "    W_HEURISTIC_FINAL = 1.0 - W_MODEL_TO_HEURISTIC # 0.1\n",
    "    \n",
    "    for i, pid in tqdm(enumerate(test_pids), total=len(test_pids), desc=\"Final Heuristic Blending\"):\n",
    "        tokens = clean_text_to_tokens(test_corpus.get(pid, \"\"), stopwords)\n",
    "        heuristic_scores_raw = score_text_with_tfidf(\n",
    "            pid, tokens, keyword_map, parent_map, child_map, tfidf_ranking_map, w_emotion=0.0\n",
    "        )\n",
    "        \n",
    "        heuristic_scores = np.zeros(num_classes)\n",
    "        for cid, score in heuristic_scores_raw.items():\n",
    "            if cid < num_classes: heuristic_scores[cid] = score\n",
    "\n",
    "        if np.max(heuristic_scores) > 0:\n",
    "            heuristic_scores = heuristic_scores / np.max(heuristic_scores)\n",
    "            \n",
    "        # ìµœì¢… ìœµí•©: Model * 0.9 + Heuristic * 0.1\n",
    "        final_hybrid_probas[i] = (W_MODEL_TO_HEURISTIC * decision_scores_hybrid[i]) + (W_HEURISTIC_FINAL * heuristic_scores)\n",
    "\n",
    "\n",
    "    decision_scores_refined = apply_gnn_score_refinement(final_hybrid_probas, G, num_classes)\n",
    "    \n",
    "    pred_labels = []\n",
    "    for i in tqdm(range(X_PCA_TEST.shape[0]), desc=\"  Selecting Top-3 and Saving\"):\n",
    "        row = decision_scores_refined[i]\n",
    "        sorted_scores = sorted(enumerate(row), key=lambda x: -x[1])\n",
    "        score_dict = {cid: sc for cid, sc in sorted_scores}\n",
    "        \n",
    "        # 1. ìµœì†Œ ë ˆì´ë¸” ì œì•½ ê¸°ë°˜ ì„ íƒ\n",
    "        selected_initial = refine_labels_with_min_constraint(\n",
    "            sorted_scores, CONFIDENCE_THRESHOLD, min_labels=MIN_LABELS, max_labels=MAX_LABELS\n",
    "        )\n",
    "        \n",
    "        # 2. ê³„ì¸µì  ì¼ê´€ì„± ê¸°ë°˜ ë³´ì • ë° ìµœëŒ€ ë ˆì´ë¸” ìˆ˜ ì¬ì¡°ì •\n",
    "        selected = enforce_hierarchical_consistency(\n",
    "            selected_initial, parent_map, score_dict, MAX_LABELS\n",
    "        )\n",
    "        \n",
    "        pred_labels.append(selected)\n",
    "\n",
    "    print(f\"\\n[SAVE] ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥ ì¤‘: {final_submission_path}\")\n",
    "    with open(final_submission_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f); writer.writerow([\"id\", \"label\"])\n",
    "        for pid, labs in tqdm(zip(test_pids, pred_labels), total=len(test_pids), desc=\"Writing Submission CSV\"):\n",
    "            writer.writerow([pid, \",\".join(map(str, labs))])\n",
    "    print(\"Submission saved âœ…\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ğŸš¨ğŸš¨ğŸš¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ì—¬ê¸°ì„œ ì¡°ì ˆí•˜ì„¸ìš”) ğŸš¨ğŸš¨ğŸš¨\n",
    "    # ğŸš¨ 1. LogReg ì—­í•  ì¶•ì†Œ (MLPì— ì§‘ì¤‘)\n",
    "    W_MLP_RATIO = 0.75       #95\n",
    "    W_LOGREG_RATIO = 0.25     #5\n",
    "    \n",
    "    # ğŸš¨ 2. Taxo ìœµí•© ê°•í™” (ëª¨ë¸ë³´ë‹¤ Taxo ì§€ì‹ì— ë” ë§ì€ ì˜í–¥ë ¥)\n",
    "    ALPHA_MLP_TAXO = 0.25     \n",
    "    \n",
    "    MLP_CONFIDENCE_THRESHOLD = 0.85 \n",
    "    W_MODEL_TO_HEURISTIC_FINAL = 0.9 \n",
    "\n",
    "    # ğŸš¨ 3. MLP êµ¬ì¡° ë³µì¡ë„ ì¦ê°€\n",
    "    MLP_HIDDEN_LAYERS = (256, 128) \n",
    "\n",
    "\n",
    "    print(\"=== START: Integrated Hybrid Pipeline (Arch Upgraded & LogReg Reduced) ===\")\n",
    "\n",
    "    # 1. Resources Load\n",
    "    name2id = load_class_names(CLASS_PATH)\n",
    "    keyword_map = load_keywords(KEYWORD_PATH, name2id)\n",
    "    parent_map, child_map, G = load_hierarchy(HIER_PATH, name2id) \n",
    "    \n",
    "    TFIDF_RANKING_MAP = load_tfidf_ranking(TFIDF_RANKING_PATH)\n",
    "    print(f\"[SETUP] Loaded TFIDF Ranking Map for {len(TFIDF_RANKING_MAP)} documents.\")\n",
    "\n",
    "    # 2. Run Integrated Pipeline\n",
    "    run_integrated_hybrid_pipeline(\n",
    "        TRAIN_CORPUS, TEST_CORPUS, STRONG_SILVER_PATH, TRAIN_SILVER_PATH,\n",
    "        name2id, keyword_map, parent_map, child_map, G, FINAL_SUBMISSION,\n",
    "        TFIDF_RANKING_MAP, \n",
    "        w_mlp_ratio=W_MLP_RATIO, w_logreg_ratio=W_LOGREG_RATIO, \n",
    "        alpha_mlp_taxo=ALPHA_MLP_TAXO,\n",
    "        top_k=3,\n",
    "        mlp_confidence_threshold=MLP_CONFIDENCE_THRESHOLD,\n",
    "        W_MODEL_TO_HEURISTIC=W_MODEL_TO_HEURISTIC_FINAL, # 0.9 ì „ë‹¬\n",
    "        MLP_HIDDEN_LAYERS=MLP_HIDDEN_LAYERS # ìƒˆ íŒŒë¼ë¯¸í„° ì „ë‹¬\n",
    "    )\n",
    "    ##ì´ê±¸ë¡œ í• ì§€ë‘"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
